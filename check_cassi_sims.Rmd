---
title: "cassi-thesis-sims"
author: "Dr. L and Cassi"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE}
# Load packages
library(rineq) ## for CI calculations 
library(tidyverse) ## for data wrangling, etc

# Be reproducible queens
set.seed(205)

# Define colors 
cols = c("#ff99ff", "#8bdddb", "#787ff6", "#ffbd59", "#7dd5f6", "#ff884d")
```

# Data Generation

We simulate samples of $n$ patients' data in the following way. 

  1.  $Z \sim Bernoulli(\pi = 0.2)$
  2.  $X|Z \sim Normal(\mu = 1 + 4Z, \sigma = 1)$
  3.  $R = \frac{Rank(X)-1}{n} + \frac{1}{2n}$
  3.  $Y|R \sim Normal(\mu = \alpha_1 + \beta_1R, \sigma = 1)$, where $\alpha_1$ and $\beta_1$ were chosen via grid search to balance (i) achieving particular values of the true concentration index and (ii) ensuring that the health outcome $Y$ remains nonnegative. 
  4.  $X^* = X + U$, where $U \sim Normal(\mu = 0, \sigma = 0.5)$
  5.  $R^* = \frac{Rank(X^*)-1}{n} + \frac{1}{2n}$
  6.  $W = R^* - R$, such that an additive measurement error is assumed for the error-prone ranks: $R^* = R + W$. 

The function below simulates data following this schema. 

```{r}
# Function to simulate data- choose variance of errors, sample size, and coefficients for model (to simulate CI magnitude)
sim_ef_data = function(sigmaU, n, alpha1, beta1) {
  # 1) Rural/not rural indicator 
  Z = rbinom(n = n, size = 1, prob = 0.2)
  
  # 2) True proximity | Rural / not rural 
  X = rnorm(n = n, mean = 1 + 4 * Z, sd = 1)
  
  # Calculate rank
  R = (rank(X) - 1)/n + 1/(2 * n)
  
  # 3) Dietary inflammation score | Fractional rank of true proximity
  eps = rnorm(n = n, mean = 0, sd = 1)
  Y = alpha1 + beta1 * R + eps
  
  # Generate errors and error-prone X
  U = rnorm(n = n, mean = 0, sd = sigmaU)
  Xstar = X + U
  
  # Calculate error-prone rank
  Rstar = (rank(Xstar) - 1)/n + 1/(2 * n)
  w = Rstar - R
  
  # 4) Return 
  data.frame(Y, X, R, Z, U, Xstar, Rstar, w, Y_bar = mean(Y))
}
```

# Concentration index and linear regression

Consider two linear regressions relating the health outcome $Y$ to the ranks based on the error-free and error-prone food access measures, respectively: 

  1.  Error-free: $Y = \alpha_1 + \beta_1 R + \epsilon$ and 
  2.  Error-prone: $Y = \alpha_1^* + \beta_1^* R^* + \epsilon^*$, 
  
where $\epsilon$ and $\epsilon^*$ are assumed to follow their own normal distributions. (We simulated $Y$ such that $\epsilon$ follows a standard normal, specifically, but we don't know the mean or standard deviation of $\epsilon^*$ necessarily.)

Importantly, these two models admit their own *expected* concentration indices: 

  1.  Error-free: $C = \left(\frac{2\sigma_R^2}{\mu_Y}\right)\beta_1 = \left(\frac{2\sigma_R^2}{\alpha_1 + \beta_1/2}\right)\beta_1$
  2.  Error-pront: $C^* = \left(\frac{2\sigma_{R^*}^2}{\mu_Y}\right)\beta_1 = \left(\frac{2\sigma_{R^*}^2}{\alpha_1^* + \beta_1^*/2}\right)\beta_1^*$, 
  
where $\sigma_R^2 = \sigma_{R^*}^2 \approx 0.08341$ is the variance of the fractional ranks (dictated only by the sample size $n = 1000$) and $\mu_Y = \textrm{E}(Y) = \textrm{E}\left\{\textrm{E}(Y|R)\right\} = \alpha_1 + \beta_1\textrm{E}(R) = \alpha_1 + \beta_1/2$ is the mean of the health outcome using the *law of iterated expectation* to get the marginal mean from the conditional distribution. (It should equivalently hold that $\mu_Y = \textrm{E}(Y) = \textrm{E}\left\{\textrm{E}(Y|R)\right\} = \alpha_1^* + \beta_1^*\textrm{E}(R^*) = \alpha_1^* + \beta_1^*/2$.)

The function below simulates a dataset, fits each of these models to it, and then returns the estimated coefficients. 

```{r}
# Simulation to calculate the beta1/beta1* coefficients 
simulate_betas = function(sigmaU, n, alpha1, beta1) {
  data = sim_ef_data(sigmaU = sigmaU, 
                      n = n, 
                      alpha1 = alpha1, 
                      beta1 = beta1)
  mu_hat = data$Y_bar[1]
  var_r = var(data$R)
  var_w = var(data$Rstar - data$R)
  fit1 = lm(Y ~ R, data = data)
  fit2 = lm(Y ~ Rstar, data = data)
  alpha1_hat = fit1$coefficients[1] 
  beta1_hat = fit1$coefficients[2] 
  alpha1star_hat = fit2$coefficients[1] 
  beta1star_hat = fit2$coefficients[2] 
  return(data.frame(alpha1_hat, 
                    beta1_hat,
                    alpha1star_hat,
                    beta1star_hat, 
                    mu_bar = mu_hat, 
                    var_r = round(var_r, 8), 
                    var_w))
}
```

Below, we run $10,000$ replications for samples of $n = 1000$ patients under each of the following three settings: 

  1.  Negative concentration index: $\alpha_1 = 2.5$ and $\beta_1 = -3$,
  2.  Zero concentration index: $\alpha_1 = 3$ and $\beta_1 = 0$, and
  3.  Positive concentration index: $\alpha_1 = -0.5$ and $\beta_1 = 3$.

```{r, cache = TRUE}
df_low_betas = do.call(what = rbind, 
                       args = replicate(n = 10000, 
                                        expr = simulate_betas(sigmaU = 0.5, 
                                                              n = 1000, 
                                                              alpha1 = 2.5, 
                                                              beta1 = -3), 
                                        simplify = FALSE)) 
df_zero_betas = do.call(what = rbind, 
                        args = replicate(n = 10000, 
                                         expr = simulate_betas(sigmaU = 0.5, 
                                                               n = 1000, 
                                                               alpha1 = 3, 
                                                               beta1 = 0), 
                                         simplify = FALSE)) 
df_high_betas = do.call(what = rbind, 
                        args = replicate(n = 10000, 
                                         expr = simulate_betas(sigmaU = 0.5, 
                                                               n = 1000, 
                                                               alpha1 = -0.5, 
                                                               beta1 = 3), 
                                         simplify = FALSE)) 
```

Then, we stack the results from these three settings so that we can visualize the results together. 

```{r}
df_betas = df_low_betas |> 
  mutate(est_ci_mu = 2 * var_r / mu_bar * beta1_hat, 
         est_ci_alpha_beta = 2 * var_r / (alpha1_hat + (beta1_hat / 2)) * beta1_hat, 
         est_ci_mu_star = 2 * var_r / mu_bar * beta1star_hat, 
         est_ci_alpha_beta_star = 2 * var_r / (alpha1star_hat + (beta1star_hat / 2)) * beta1star_hat, 
         approx_ci = -0.5, 
         alpha1 = 2.5, 
         beta1 = -3, 
         exp_mu = alpha1 + beta1 * 0.5, 
         exp_ci_mu = 2 * var_r / exp_mu * beta1, 
         exp_ci_alpha_beta = 2 * var_r / (alpha1 + (beta1 / 2)) * beta1) |> 
  bind_rows(
    df_zero_betas |> 
      mutate(est_ci_mu = 2 * var_r / mu_bar * beta1_hat, 
             est_ci_alpha_beta = 2 * var_r / (alpha1_hat + (beta1_hat / 2)) * beta1_hat, 
             est_ci_mu_star = 2 * var_r / mu_bar * beta1star_hat, 
             est_ci_alpha_beta_star = 2 * var_r / (alpha1star_hat + (beta1star_hat / 2)) * beta1star_hat, 
             approx_ci = 0.0, 
             alpha1 = 3, 
             beta1 = 0, 
             exp_mu = alpha1 + beta1 * 0.5, 
             exp_ci_mu = 2 * var_r / exp_mu * beta1, 
             exp_ci_alpha_beta = 2 * var_r / (alpha1 + (beta1 / 2)) * beta1) 
  ) |> 
  bind_rows(
    df_high_betas |> 
      mutate(est_ci_mu = 2 * var_r / mu_bar * beta1_hat, 
             est_ci_alpha_beta = 2 * var_r / (alpha1_hat + (beta1_hat / 2)) * beta1_hat, 
             est_ci_mu_star = 2 * var_r / mu_bar * beta1star_hat, 
             est_ci_alpha_beta_star = 2 * var_r / (alpha1star_hat + (beta1star_hat / 2)) * beta1star_hat, 
             approx_ci = 0.5, 
             alpha1 = -0.5, 
             beta1 = 3, 
             exp_mu = alpha1 + beta1 * 0.5, 
             exp_ci_mu = 2 * var_r / exp_mu * beta1, 
             exp_ci_alpha_beta = 2 * var_r / (alpha1 + (beta1 / 2)) * beta1) 
  )
```

# Results {.tabset}

## Both concentration index calculations (using $\bar{Y}$ or $$\hat{\alpha}_1$ and $\hat{\beta}_1$) were empirically unbiased and equal to each other when using ranks based on error-free $X$ 

```{r}
# CHECK: expected CI based on mu or alpha/beta are equal
with(df_betas, table(exp_ci_mu == exp_ci_alpha_beta)) 

# CHECK: estimated CI based on mu or alpha/beta are not exactly equal 
with(df_betas, table(est_ci_mu == est_ci_alpha_beta)) 
## (but they're within 15 decimal places...)
with(df_betas, summary(est_ci_mu - est_ci_alpha_beta)) 
```

```{r, echo = FALSE}
# Plot  
df_betas |> 
  ggplot(aes(x = factor(exp_ci_mu), 
             y = est_ci_mu, 
             group = factor(exp_ci_mu), 
             fill = factor(exp_ci_mu))) + 
  geom_boxplot() + 
  geom_hline(aes(yintercept = exp_ci_mu), 
             linetype = "dashed", 
             color = cols[4]) + 
  theme_minimal() + 
  scale_fill_manual(values = cols, 
                    guide = "none") + 
  labs(x = "True Concentration Index", 
       y = "Estimated Concentration Index (Using Ybar and X)")
```

```{r, echo = FALSE}
# Plot  
df_betas |> 
  ggplot(aes(x = factor(exp_ci_mu), 
             y = est_ci_alpha_beta, 
             group = factor(exp_ci_mu), 
             fill = factor(exp_ci_mu))) + 
  geom_boxplot() + 
  geom_hline(aes(yintercept = exp_ci_mu), 
             linetype = "dashed", 
             color = cols[4]) + 
  theme_minimal() + 
  scale_fill_manual(values = cols, 
                    guide = "none") + 
  labs(x = "True Concentration Index", 
       y = "Estimated Concentration Index (Using alpha1, beta1, and X)")
```

```{r, echo = FALSE}
# Plot  
df_betas |> 
  ggplot(aes(x = est_ci_mu, 
             y = est_ci_alpha_beta, 
             color = factor(exp_ci_mu))) + 
  geom_point() + 
  geom_abline(intercept = 0, 
              slope = 1, 
              linetype = "dashed", 
              color = cols[4]) + 
  theme_minimal() + 
  scale_color_manual(values = cols, 
                     guide = "none") + 
  labs(x = "Estimated Concentration Index (Using mu and X)", 
       y = "Estimated Concentration Index (Using alpha1, beta1, and X)")
```

## Both concentration index calculations (using $\bar{Y}$ or $$\hat{\alpha}_1$ and $\hat{\beta}_1$) were empirically biased but equal to each other when using ranks based on error-prone $X^*$ 

```{r}
# CHECK: estimated CI based on mu or alpha/beta are not exactly equal 
with(df_betas, table(est_ci_mu_star == est_ci_alpha_beta_star)) 
## (but they're within 15 decimal places...)
with(df_betas, summary(est_ci_mu_star - est_ci_alpha_beta_star)) 
```

```{r, echo = FALSE}
# Plot  
df_betas |> 
  ggplot(aes(x = factor(exp_ci_mu), 
             y = est_ci_mu_star, 
             group = factor(exp_ci_mu), 
             fill = factor(exp_ci_mu))) + 
  geom_boxplot() + 
  geom_hline(aes(yintercept = exp_ci_mu), 
             linetype = "dashed", 
             color = cols[4]) + 
  theme_minimal() + 
  scale_fill_manual(values = cols, 
                    guide = "none") + 
  labs(x = "True Concentration Index", 
       y = "Estimated Concentration Index (Using Ybar and X*)")
```

```{r, echo = FALSE}
# Plot  
df_betas |> 
  ggplot(aes(x = factor(exp_ci_mu), 
             y = est_ci_alpha_beta_star, 
             group = factor(exp_ci_mu), 
             fill = factor(exp_ci_mu))) + 
  geom_boxplot() + 
  geom_hline(aes(yintercept = exp_ci_mu), 
             linetype = "dashed", 
             color = cols[4]) + 
  theme_minimal() + 
  scale_fill_manual(values = cols, 
                    guide = "none") + 
  labs(x = "True Concentration Index", 
       y = "Estimated Concentration Index (Using alpha1, beta1, and X*)")
```

```{r, echo = FALSE}
# Plot  
df_betas |> 
  ggplot(aes(x = est_ci_mu_star, 
             y = est_ci_alpha_beta_star, 
             color = factor(exp_ci_mu))) + 
  geom_point() + 
  geom_abline(intercept = 0, 
              slope = 1, 
              linetype = "dashed", 
              color = cols[4]) + 
  theme_minimal() + 
  scale_color_manual(values = cols, 
                     guide = "none") + 
  labs(x = "Estimated Concentration Index (Using mu and X*)", 
       y = "Estimated Concentration Index (Using alpha1, beta1, and X*)")
```

## Attenuation (Assuming Independence Between $R$ and $U$)

Our formula, which agrees with the well-established attenuation factor in the measurement error literature, was: $$\beta_1^*=\beta_1\left\{\frac{\textrm{Var}(R)}{\textrm{Var}(R)+\textrm{Var}(W)}\right\},$$ where we often let $\lambda = \frac{\textrm{Var}(R)}{\textrm{Var}(R)+\textrm{Var}(W)}$ denote the *attenuation factor*. This formula assumes that the magnitude of the error $W$ is independent of the original error-free rank $R$, such that $\textrm{Cov}(R,W) = 0$. 

```{r}
df_betas = df_betas |> 
  mutate(lambda_hat = beta1star_hat / beta1_hat) 
```

```{r}
df_betas |> 
  ggplot(aes(x = beta1_hat, y = beta1star_hat, color = factor(exp_ci_mu))) + 
  geom_point(alpha = 0.1) + 
  geom_smooth(color = "black") + 
  geom_abline(intercept = 0, 
              slope = 1, 
              color = cols[4], 
              linetype = "dashed") + 
  geom_vline(aes(xintercept = beta1), 
             color = cols[5], 
             linetype = "dashed") + 
  theme_minimal() + 
  scale_color_manual(values = cols, 
                     guide = "none") +
  facet_wrap(~factor(exp_ci_mu), scales = "free")
```

```{r}
atten_fit_low = lm(formula = beta1star_hat ~ beta1_hat, 
                   data = df_betas,
                   subset = exp_ci_mu == -0.50050002)

atten_fit_zero = lm(formula = beta1star_hat ~ beta1_hat, 
                   data = df_betas,
                   subset = exp_ci_mu == 0)

atten_fit_high = lm(formula = beta1star_hat ~ beta1_hat, 
                    data = df_betas,
                    subset = exp_ci_mu == 0.50050002)
```

Based on the linear regression between $\hat{\beta}_1^*$ and $\hat{\beta}_1$, across all replications, the average attenuation factors were estimated to be: 

  1.  When $C = -0.5$, $\hat{\lambda} =$ `r atten_fit_low$coefficients[2]`, 
  2.  When $C = 0.0$, $\hat{\lambda} =$ `r atten_fit_zero$coefficients[2]`, and 
  3.  When $C = 0.5$, $\hat{\lambda} =$ `r atten_fit_high$coefficients[2]`. 
 
```{r}
atten_fit_low = lm(formula = est_ci_mu_star ~ est_ci_mu, 
                   data = df_betas,
                   subset = exp_ci_mu == -0.50050002)

atten_fit_zero = lm(formula = est_ci_mu_star ~ est_ci_mu, 
                   data = df_betas,
                   subset = exp_ci_mu == 0)

atten_fit_high = lm(formula = est_ci_mu_star ~ est_ci_mu, 
                    data = df_betas,
                    subset = exp_ci_mu == 0.50050002)
```
  
Based on the estimated concentration indices $\hat{C}^*$ and $\hat{C}$, across all replications, the average attenuation factors were estimated to be: 

  1.  When $C = -0.5$, $\hat{\lambda} =$ `r atten_fit_low$coefficients[2]`, 
  2.  When $C = 0.0$, $\hat{\lambda} =$ `r atten_fit_zero$coefficients[2]`, 
  3.  When $C = 0.5$, $\hat{\lambda} =$ `r atten_fit_high$coefficients[2]`, 

```{r}
# Calculate variance of the ranks R based on error-free X
table(df_betas$var_r)

# Variance of the errors W in the error-prone ranks were almost all unique
length(unique(df_betas$var_w))
## They varied across replications, ranging from 0.007187 to 0.014114
### They were fairly symmetric, since median and mean are both approximately 0.0101
summary(df_betas$var_w) 
```

Finally, based on our formula, we expected the following attenuation factor: 

  1.  For all true $C$, $\lambda = \frac{\sigma_R^2}{\sigma_R^2 + \sigma_W^2} = \frac{0.08341667}{0.08341667 + 0.0101}$, where $\sigma_R^2 = 0.08341667$ is dictated only by the same size (so it doesn't vary between replications) and $\sigma_W^2 = 0.0101$ is the average variance of the errors in the ranks across all replications. 
  
```{r}
varR = unique(df_betas$var_r) ## variance of the true ranks 
varW = mean(df_betas$var_w) ## (mean) variance of the errors in ranks
exp_lambda = (varR) / (varR + varW)
exp_lambda
```

We can also calculate the empirical attenuation factor $\tilde{\lambda} = \hat{C}^*/\hat{C}$ for each replication and compare its distribution to the expected attenuation factor of $\lambda =$ `r exp_lambda` based on our formula.

```{r}
# Calculate empirical attenuation factor for each replicate
df_betas = df_betas |> 
  mutate(lambda_tilde = est_ci_mu_star / est_ci_mu)

# Plot their distributions against the expected 
df_betas |> 
  ggplot(aes(x = lambda_tilde, fill = factor(exp_ci_mu))) + 
  geom_histogram() + 
  geom_vline(aes(xintercept = exp_lambda), 
             linetype = "dashed", 
             color = cols[4]) + 
  theme_minimal() + 
  scale_fill_manual(values = cols, 
                    guide = "none") + 
  facet_wrap(~factor(exp_ci_mu), 
             scales = "free")
```

## Potential Correlation/Covariance 

```{r}
# Simulation to calculate the correlation between W and R
simulate_corr = function(sigmaU, n, alpha1, beta1) {
  data = sim_ef_data(sigmaU = sigmaU, 
                      n = n, 
                      alpha1 = alpha1, 
                      beta1 = beta1)
  data$W = data$Rstar - data$R
  return(data.frame(var_r = round(var(data$R), 8), 
                    cor = cor(x = data$W, y = data$R), 
                    cov = cov(x = data$W, y = data$R)))
}
```

```{r, cache = TRUE}
df_low_corr = do.call(what = rbind, 
                      args = replicate(n = 10000, 
                                       expr = simulate_corr(sigmaU = 0.5, 
                                                            n = 1000, 
                                                            alpha1 = 2.5, 
                                                            beta1 = -3), 
                                       simplify = FALSE)) 
df_zero_corr = do.call(what = rbind, 
                       args = replicate(n = 10000, 
                                        expr = simulate_corr(sigmaU = 0.5, 
                                                             n = 1000, 
                                                             alpha1 = 3, 
                                                             beta1 = 0), 
                                        simplify = FALSE)) 
df_high_corr = do.call(what = rbind, 
                       args = replicate(n = 10000, 
                                        expr = simulate_corr(sigmaU = 0.5, 
                                                             n = 1000, 
                                                             alpha1 = -0.5, 
                                                             beta1 = 3), 
                                        simplify = FALSE)) 
```

```{r}
df_corr = df_low_corr |> 
  mutate(alpha1 = 2.5, 
         beta1 = -3, 
         exp_mu = alpha1 + beta1 * 0.5, 
         exp_ci_mu = 2 * var_r / exp_mu * beta1) |> 
  bind_rows(
    df_zero_corr |> 
      mutate(alpha1 = 3, 
             beta1 = 0, 
             exp_mu = alpha1 + beta1 * 0.5, 
             exp_ci_mu = 2 * var_r / exp_mu * beta1) 
  ) |> 
  bind_rows(
    df_high_corr |> 
      mutate(alpha1 = -0.5, 
             beta1 = 3, 
             exp_mu = alpha1 + beta1 * 0.5, 
             exp_ci_mu = 2 * var_r / exp_mu * beta1) 
  )
```

```{r, echo = FALSE, message = FALSE}
df_corr |> 
  ggplot(aes(x = cor, fill = factor(exp_ci_mu))) + 
  geom_histogram() + 
  theme_minimal() + 
  scale_fill_manual(values = cols, 
                    name = "True\nConcentration\nIndex:") + 
  labs(x = "Correlation between Error-Free Ranks R and Errors W", 
       y = "Number of Replications")

df_corr |> 
  ggplot(aes(x = cov, fill = factor(exp_ci_mu))) + 
  geom_histogram() + 
  theme_minimal() + 
  scale_fill_manual(values = cols, 
                    name = "True\nConcentration\nIndex:") + 
  labs(x = "Covariance between Error-Free Ranks R and Errors W", 
       y = "Number of Replications")
```

## Attenuation (Allowing Dependence Between $R$ and $U$)

Since the ranks $R$ and $R^*$ are both constrained to be between $0$ and $1$, 
$$0 \leq R \leq 1 \text{ and } 0 \leq R^* \leq 1,$$
which further implies that $$0 \leq R + W \leq 1 \rightarrow -R \leq W \leq R.$$ From this inequality, we can see that the values of $W$ are bounded above and below by $R$, making the usual assumption of independence unreasonable here. 

*Note: This is interesting! Because when we originally added the errors $U$ to $X$, we know that they were independent. However, the conversion from $X$ and $X^*$ to the ranks $R$ and $R^*$ "undo" that assumption of independence.*

Thus, we can revisit our formula for the attenuation factor $\lambda$, and go back to before we assumed independence:  
$$\beta_1^* = \frac{\beta_1 \textrm{Var}(R) + \beta_1 \textrm{Cov}(R, W)}{\textrm{Var}(R) + \textrm{Var}(W) + 2\textrm{Cov}(R,W)} = \beta_1\left\{\frac{\textrm{Var}(R) + \textrm{Cov}(R, W)}{\textrm{Var}(R) + \textrm{Var}(W) + 2\textrm{Cov}(R,W)}\right\},$$
where we still assume that $R \perp \epsilon$ and $W \perp \epsilon$, and the *new attenuation factor* (relaxing the assumption of independence between $R$ and $W$) is $\lambda = \frac{\textrm{Var}(R) + \textrm{Cov}(R, W)}{\textrm{Var}(R) + \textrm{Var}(W) + 2\textrm{Cov}(R,W)}$. 

We run new simulations to check this formula, saving $\textrm{Cov}(R,W)$ from each replication. 

```{r}
# Simulation to calculate the beta1/beta1* coefficients 
simulate_betas = function(sigmaU, n, alpha1, beta1) {
  data = sim_ef_data(sigmaU = sigmaU, 
                      n = n, 
                      alpha1 = alpha1, 
                      beta1 = beta1)
  data$W = data$Rstar - data$R
  mu_hat = data$Y_bar[1]
  var_r = var(data$R)
  var_w = var(data$W)
  cov_rw = cov(data$R, data$W)
  fit1 = lm(Y ~ R, data = data)
  fit2 = lm(Y ~ Rstar, data = data)
  alpha1_hat = fit1$coefficients[1] 
  beta1_hat = fit1$coefficients[2] 
  alpha1star_hat = fit2$coefficients[1] 
  beta1star_hat = fit2$coefficients[2] 
  return(data.frame(alpha1_hat, 
                    beta1_hat,
                    alpha1star_hat,
                    beta1star_hat, 
                    mu_bar = mu_hat, 
                    var_r = round(var_r, 8), 
                    var_w, 
                    cov_rw))
}
```

```{r, cache = TRUE, echo = FALSE}
df_low_betas = do.call(what = rbind, 
                       args = replicate(n = 10000, 
                                        expr = simulate_betas(sigmaU = 0.5, 
                                                              n = 1000, 
                                                              alpha1 = 2.5, 
                                                              beta1 = -3), 
                                        simplify = FALSE)) 
df_zero_betas = do.call(what = rbind, 
                        args = replicate(n = 10000, 
                                         expr = simulate_betas(sigmaU = 0.5, 
                                                               n = 1000, 
                                                               alpha1 = 3, 
                                                               beta1 = 0), 
                                         simplify = FALSE)) 
df_high_betas = do.call(what = rbind, 
                        args = replicate(n = 10000, 
                                         expr = simulate_betas(sigmaU = 0.5, 
                                                               n = 1000, 
                                                               alpha1 = -0.5, 
                                                               beta1 = 3), 
                                         simplify = FALSE)) 

df_betas = df_low_betas |> 
  mutate(est_ci_mu = 2 * var_r / mu_bar * beta1_hat, 
         est_ci_alpha_beta = 2 * var_r / (alpha1_hat + (beta1_hat / 2)) * beta1_hat, 
         est_ci_mu_star = 2 * var_r / mu_bar * beta1star_hat, 
         est_ci_alpha_beta_star = 2 * var_r / (alpha1star_hat + (beta1star_hat / 2)) * beta1star_hat, 
         approx_ci = -0.5, 
         alpha1 = 2.5, 
         beta1 = -3, 
         exp_mu = alpha1 + beta1 * 0.5, 
         exp_ci_mu = 2 * var_r / exp_mu * beta1, 
         exp_ci_alpha_beta = 2 * var_r / (alpha1 + (beta1 / 2)) * beta1) |> 
  bind_rows(
    df_zero_betas |> 
      mutate(est_ci_mu = 2 * var_r / mu_bar * beta1_hat, 
             est_ci_alpha_beta = 2 * var_r / (alpha1_hat + (beta1_hat / 2)) * beta1_hat, 
             est_ci_mu_star = 2 * var_r / mu_bar * beta1star_hat, 
             est_ci_alpha_beta_star = 2 * var_r / (alpha1star_hat + (beta1star_hat / 2)) * beta1star_hat, 
             approx_ci = 0.0, 
             alpha1 = 3, 
             beta1 = 0, 
             exp_mu = alpha1 + beta1 * 0.5, 
             exp_ci_mu = 2 * var_r / exp_mu * beta1, 
             exp_ci_alpha_beta = 2 * var_r / (alpha1 + (beta1 / 2)) * beta1) 
  ) |> 
  bind_rows(
    df_high_betas |> 
      mutate(est_ci_mu = 2 * var_r / mu_bar * beta1_hat, 
             est_ci_alpha_beta = 2 * var_r / (alpha1_hat + (beta1_hat / 2)) * beta1_hat, 
             est_ci_mu_star = 2 * var_r / mu_bar * beta1star_hat, 
             est_ci_alpha_beta_star = 2 * var_r / (alpha1star_hat + (beta1star_hat / 2)) * beta1star_hat, 
             approx_ci = 0.5, 
             alpha1 = -0.5, 
             beta1 = 3, 
             exp_mu = alpha1 + beta1 * 0.5, 
             exp_ci_mu = 2 * var_r / exp_mu * beta1, 
             exp_ci_alpha_beta = 2 * var_r / (alpha1 + (beta1 / 2)) * beta1) 
  )
```

```{r}
# Covariance of the true ranks R and errors W were almost all unique
length(unique(df_betas$cov_rw))
## They varied across replications, ranging from -0.007079 to -0.003501 
### They were fairly symmetric, since median and mean are both approximately -0.0050
summary(df_betas$cov_rw) 
```

Based on our new formula, we expected the following attenuation factor: 

  1.  For all true $C$, $\lambda = \frac{\sigma_R^2 + \sigma_{RW}}{\sigma_R^2 + \sigma_W^2 + 2\sigma_{RW}} = \frac{0.08341667}{0.08341667 + 0.0101}$, where (again) $\sigma_R^2 = 0.08341667$ is dictated only by the same size (so it doesn't vary between replications), $\sigma_W^2 = 0.0101$ is the average variance of the errors in the ranks across all replications, and (new) $\sigma_{RW} = -0.0050$ is the average covariance between the errors and the ranks across all replications. 

```{r}
varR = unique(df_betas$var_r) ## variance of the true ranks 
varW = mean(df_betas$var_w) ## (mean) variance of the errors in ranks
covRW = mean(df_betas$cov_rw) ## (mean) covariance between true ranks and errors
exp_lambda2 = (varR + covRW) / (varR + varW + 2 * covRW)
exp_lambda2
```

We can also calculate the empirical attenuation factor $\tilde{\lambda} = \hat{C}^*/\hat{C}$ for each replication and compare its distribution to the expected attenuation factor of $\lambda =$ `r exp_lambda` based on our formula.

```{r, echo = FALSE, message = FALSE}
# Calculate empirical attenuation factor for each replicate
df_betas = df_betas |> 
  mutate(lambda_tilde = est_ci_mu_star / est_ci_mu)

# Plot their distributions against the expected 
df_betas |> 
  ggplot(aes(x = lambda_tilde, fill = factor(exp_ci_mu))) + 
  geom_histogram() + 
  geom_vline(aes(xintercept = exp_lambda), 
             linetype = "dashed", 
             color = cols[4]) + 
  geom_vline(aes(xintercept = exp_lambda2), 
             linetype = "dashed", 
             color = cols[5]) + 
  theme_minimal() + 
  scale_fill_manual(values = cols, 
                    guide = "none") + 
  facet_wrap(~factor(exp_ci_mu), 
             scales = "free")
```

## Moment-Based Corrections for Partially Validated Data 

```{r}
# Simulation to calculate the beta1/beta1* coefficients 
simulate_moment_correct = function(sigmaU, n, alpha1, beta1, v = 0.25) {
  data = sim_ef_data(sigmaU = sigmaU, 
                      n = n, 
                      alpha1 = alpha1, 
                      beta1 = beta1)
  data$W = data$Rstar - data$R
  mu_hat = data$Y_bar[1]
  var_r = var(data$R)
  var_w = var(data$W)
  cov_rw = cov(data$R, data$W)
  fit1 = lm(Y ~ R, data = data)
  fit2 = lm(Y ~ Rstar, data = data)
  alpha1_hat = fit1$coefficients[1] 
  beta1_hat = fit1$coefficients[2] 
  alpha1star_hat = fit2$coefficients[1] 
  beta1star_hat = fit2$coefficients[2] 
  
  est_ci_mu = 2 * var_r / mu_hat * beta1_hat ## theoretically corrected
  est_ci_mu_star = 2 * var_r / mu_hat * beta1star_hat ## naive 
  
  ## Mimic two-phase design we
  nv = n * v 
  data$V = as.numeric(1:n %in% sample(1:n, nv, replace = FALSE)) ### SRS 
  var_r = var(data$R[data$V == 1]) ## variance of the error-prone ranks 
  varW = mean(data$W[data$V == 1]) ## (mean) variance of the errors in ranks
  covRW = cov(data$Rstar[data$V == 1], data$Rstar[data$V == 1]) ## (mean) covariance between true ranks and errors
  exp_lambda2 = (varR + covRW) / (varR + varW + 2 * covRW)
  exp_lambda2
  
  return(data.frame(alpha1_hat, 
                    beta1_hat,
                    alpha1star_hat,
                    beta1star_hat, 
                    mu_bar = mu_hat, 
                    var_r = round(var_r, 8), 
                    var_w, 
                    cov_rw))
}
```

```{r, cache = TRUE, echo = FALSE}
df_low_betas = do.call(what = rbind, 
                       args = replicate(n = 10000, 
                                        expr = simulate_betas(sigmaU = 0.5, 
                                                              n = 1000, 
                                                              alpha1 = 2.5, 
                                                              beta1 = -3), 
                                        simplify = FALSE)) 
df_zero_betas = do.call(what = rbind, 
                        args = replicate(n = 10000, 
                                         expr = simulate_betas(sigmaU = 0.5, 
                                                               n = 1000, 
                                                               alpha1 = 3, 
                                                               beta1 = 0), 
                                         simplify = FALSE)) 
df_high_betas = do.call(what = rbind, 
                        args = replicate(n = 10000, 
                                         expr = simulate_betas(sigmaU = 0.5, 
                                                               n = 1000, 
                                                               alpha1 = -0.5, 
                                                               beta1 = 3), 
                                         simplify = FALSE)) 

df_betas = df_low_betas |> 
  mutate(est_ci_mu = 2 * var_r / mu_bar * beta1_hat, 
         est_ci_alpha_beta = 2 * var_r / (alpha1_hat + (beta1_hat / 2)) * beta1_hat, 
         est_ci_mu_star = 2 * var_r / mu_bar * beta1star_hat, 
         est_ci_alpha_beta_star = 2 * var_r / (alpha1star_hat + (beta1star_hat / 2)) * beta1star_hat, 
         approx_ci = -0.5, 
         alpha1 = 2.5, 
         beta1 = -3, 
         exp_mu = alpha1 + beta1 * 0.5, 
         exp_ci_mu = 2 * var_r / exp_mu * beta1, 
         exp_ci_alpha_beta = 2 * var_r / (alpha1 + (beta1 / 2)) * beta1) |> 
  bind_rows(
    df_zero_betas |> 
      mutate(est_ci_mu = 2 * var_r / mu_bar * beta1_hat, 
             est_ci_alpha_beta = 2 * var_r / (alpha1_hat + (beta1_hat / 2)) * beta1_hat, 
             est_ci_mu_star = 2 * var_r / mu_bar * beta1star_hat, 
             est_ci_alpha_beta_star = 2 * var_r / (alpha1star_hat + (beta1star_hat / 2)) * beta1star_hat, 
             approx_ci = 0.0, 
             alpha1 = 3, 
             beta1 = 0, 
             exp_mu = alpha1 + beta1 * 0.5, 
             exp_ci_mu = 2 * var_r / exp_mu * beta1, 
             exp_ci_alpha_beta = 2 * var_r / (alpha1 + (beta1 / 2)) * beta1) 
  ) |> 
  bind_rows(
    df_high_betas |> 
      mutate(est_ci_mu = 2 * var_r / mu_bar * beta1_hat, 
             est_ci_alpha_beta = 2 * var_r / (alpha1_hat + (beta1_hat / 2)) * beta1_hat, 
             est_ci_mu_star = 2 * var_r / mu_bar * beta1star_hat, 
             est_ci_alpha_beta_star = 2 * var_r / (alpha1star_hat + (beta1star_hat / 2)) * beta1star_hat, 
             approx_ci = 0.5, 
             alpha1 = -0.5, 
             beta1 = 3, 
             exp_mu = alpha1 + beta1 * 0.5, 
             exp_ci_mu = 2 * var_r / exp_mu * beta1, 
             exp_ci_alpha_beta = 2 * var_r / (alpha1 + (beta1 / 2)) * beta1) 
  )
```
